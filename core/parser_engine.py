# core/parser_engine.py
import requests
from bs4 import BeautifulSoup
import json
import time
import random
from typing import Optional, List, Union, Dict

class ParserEngine:
    """–ú–æ—â–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª—é–±—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
    
    def __init__(self, use_selenium=False, headless=True):
        """
        use_selenium: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Selenium –¥–ª—è JavaScript
        headless: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –æ–∫–Ω–æ –±—Ä–∞—É–∑–µ—Ä–∞
        """
        self.use_selenium = use_selenium
        self.headless = headless
        self.driver = None
        self.session = requests.Session()
        
        # –ú–∞—Å–∫–∏—Ä—É–µ–º—Å—è –ø–æ–¥ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive',
        })
    
    def load_from_url(self, url: str, retries=3, delay=1) -> Optional[BeautifulSoup]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ URL —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        retries: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        delay: –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
        """
        if self.use_selenium:
            return self._load_with_selenium(url)
        
        for attempt in range(retries):
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(random.uniform(delay, delay + 1))
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                content = response.content
                
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–æ–¥–∏—Ä–æ–≤–∫—É –≤ HTML
                import re
                charset_match = re.search(b'charset=([^"\'>\\s]+)', content[:5000])
                if charset_match:
                    encoding = charset_match.group(1).decode('ascii', 'ignore')
                    try:
                        text = content.decode(encoding)
                        print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")
                        return BeautifulSoup(text, "html.parser")
                    except:
                        pass
                
                # –ü—Ä–æ–±—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                encodings_to_try = ['utf-8', 'windows-1251', 'koi8-r', 'cp866']
                for enc in encodings_to_try:
                    try:
                        text = content.decode(enc)
                        print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}) –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ {enc}")
                        return BeautifulSoup(text, "html.parser")
                    except UnicodeDecodeError:
                        continue
                
                # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ, –ø—Ä–æ–±—É–µ–º —Å –æ—à–∏–±–∫–∞–º–∏
                text = content.decode('utf-8', errors='ignore')
                print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}) —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫")
                return BeautifulSoup(text, "html.parser")
                
            except requests.exceptions.RequestException as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(delay * 2)
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url} –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫")
                    return None
    
    def _load_with_selenium(self, url: str, wait_for=None) -> Optional[BeautifulSoup]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å JavaScript —á–µ—Ä–µ–∑ Selenium"""
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.chrome.service import Service
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from webdriver_manager.chrome import ChromeDriverManager
            
            options = Options()
            if self.headless:
                options.add_argument('--headless')
            
            # –ú–∞—Å–∫–∏—Ä—É–µ–º Selenium
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π user-agent
            options.add_argument(f'user-agent={self.session.headers["User-Agent"]}')
            
            # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--no-sandbox')
            
            print("üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä...")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            
            # –ú–∞—Å–∫–∏—Ä—É–µ–º WebDriver
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            print(f"üì± –ó–∞–≥—Ä—É–∂–∞–µ–º: {url}")
            self.driver.get(url)
            
            # –ñ–¥—ë–º –∑–∞–≥—Ä—É–∑–∫–∏ JavaScript
            if wait_for:
                try:
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, wait_for))
                    )
                    print(f"‚úÖ –≠–ª–µ–º–µ–Ω—Ç '{wait_for}' –∑–∞–≥—Ä—É–∂–µ–Ω")
                except Exception as e:
                    print(f"‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            else:
                time.sleep(3)
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–Ω–∏–∑
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            
            # –ü–æ–ª—É—á–∞–µ–º HTML
            html = self.driver.page_source
            self.driver.quit()
            
            print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å JavaScript –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return BeautifulSoup(html, "html.parser")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Selenium: {e}")
            if self.driver:
                self.driver.quit()
            return None
    
    def load_from_html(self, html_content: str) -> BeautifulSoup:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ HTML —Å—Ç—Ä–æ–∫–∏"""
        return BeautifulSoup(html_content, "html.parser")
    
    def extract_css(self, soup: BeautifulSoup, selector: str, attribute: str = None) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø–æ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—É
        attribute: –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω, –±–µ—Ä—ë–º –∞—Ç—Ä–∏–±—É—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä 'href' –¥–ª—è —Å—Å—ã–ª–æ–∫)
        """
        if soup is None:
            return []
        
        elements = soup.select(selector)
        result = []
        
        for el in elements:
            if attribute:
                # –ë–µ—Ä—ë–º –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞
                value = el.get(attribute)
                if value:
                    result.append(str(value).strip())
            else:
                # –ë–µ—Ä—ë–º —Ç–µ–∫—Å—Ç
                result.append(el.get_text(strip=True))
        
        return result
    
    def extract_xpath(self, soup: BeautifulSoup, xpath: str, attribute: str = None) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø–æ XPath
        –¢—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞: pip3 install lxml
        """
        try:
            from lxml import html
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º BeautifulSoup –≤ lxml –¥–µ—Ä–µ–≤–æ
            dom = html.fromstring(str(soup))
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ XPath
            elements = dom.xpath(xpath)
            result = []
            
            for el in elements:
                if hasattr(el, 'text') and not attribute:
                    result.append(str(el.text).strip())
                elif isinstance(el, str):
                    result.append(el.strip())
                elif attribute and hasattr(el, 'get'):
                    value = el.get(attribute)
                    if value:
                        result.append(str(value).strip())
            
            return result
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ XPath: {e}")
            return []
    
    def extract_json_next_data(self, soup: BeautifulSoup) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á—å JSON –∏–∑ __NEXT_DATA__ (React —Å–∞–π—Ç—ã)"""
        if soup is None:
            return None
        
        script = soup.find("script", id="__NEXT_DATA__")
        if not script:
            return None
        
        try:
            return json.loads(script.string)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ __NEXT_DATA__: {e}")
            return None
    
    def extract_json_ld(self, soup: BeautifulSoup) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ JSON-LD
        (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–∏–º–∏ —Å–∞–π—Ç–∞–º–∏ –¥–ª—è SEO)
        """
        if soup is None:
            return []
        
        scripts = soup.find_all("script", type="application/ld+json")
        result = []
        
        for script in scripts:
            try:
                data = json.loads(script.string)
                result.append(data)
            except:
                pass
        
        return result
    
    def extract_json_path(self, json_data: Union[Dict, List], path: str):
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON –ø–æ –ø—É—Ç–∏ –≤–∏–¥–∞:
        props.pageProps.initialState.entities.recipes.0.title
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–∞—Å—Å–∏–≤—ã –∏ —Ñ–∏–ª—å—Ç—Ä—ã
        """
        if json_data is None:
            return None
        
        try:
            parts = path.split(".")
            current = json_data
            
            for part in parts:
                if isinstance(current, list):
                    try:
                        idx = int(part)
                        if 0 <= idx < len(current):
                            current = current[idx]
                        else:
                            return None
                    except ValueError:
                        result = []
                        for item in current:
                            if isinstance(item, dict) and part in item:
                                result.append(item[part])
                        return result
                        
                elif isinstance(current, dict):
                    current = current.get(part)
                    if current is None:
                        return None
                else:
                    return None
            
            return current
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON: {e}")
            return None
    
    def extract_from_iframe(self, soup: BeautifulSoup, iframe_selector: str, inner_selector: str):
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ iframe
        iframe_selector: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è iframe
        inner_selector: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –≤–Ω—É—Ç—Ä–∏ iframe
        """
        iframes = soup.select(iframe_selector)
        result = []
        
        for iframe in iframes:
            src = iframe.get('src')
            if src and src.startswith('http'):
                iframe_soup = self.load_from_url(src)
                if iframe_soup:
                    result.extend(self.extract_css(iframe_soup, inner_selector))
        
        return result
    
    def extract_regex(self, text: str, pattern: str, group=0):
        """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é"""
        import re
        try:
            matches = re.findall(pattern, text, re.DOTALL)
            if matches:
                if group > 0 and isinstance(matches[0], tuple):
                    return [m[group-1] for m in matches]
                return matches
            return []
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ regex: {e}")
            return []
    
    def extract_all_methods(self, soup: BeautifulSoup, url: str = None):
        """
        –ò–∑–≤–ª–µ—á—å –≤—Å—ë, —á—Ç–æ –º–æ–∂–Ω–æ, –≤—Å–µ–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ä–∞–∑–≤–µ–¥–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞
        """
        result = {
            'title': self.extract_css(soup, 'title'),
            'h1': self.extract_css(soup, 'h1'),
            'meta': {},
            'links': self.extract_css(soup, 'a', 'href')[:10],
            'images': self.extract_css(soup, 'img', 'src')[:5],
            'json_ld': self.extract_json_ld(soup),
            'next_data': self.extract_json_next_data(soup)
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º meta-—Ç–µ–≥–∏
        for meta in soup.find_all('meta'):
            if meta.get('name'):
                result['meta'][meta['name']] = meta.get('content')
            elif meta.get('property'):
                result['meta'][meta['property']] = meta.get('content')
        
        return result